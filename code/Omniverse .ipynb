{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1R9jIFG3YipwlrQz2uc55JvcWCwAm4Xed","authorship_tag":"ABX9TyOBt4nXlvtPs7xiCEnqsqnY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eEY8o9ioRexm"},"outputs":[],"source":["import omni.replicator.core as rep\n","\n","# Create and configure the camera\n","camera = rep.create.camera()\n","with rep.trigger.on_frame(num_frames=20): # edit the number of frames as necessary\n","    with camera:\n","        rep.modify.pose(\n","            position=rep.distribution.uniform((-5, 5, 1), (-1, 15, 5)),\n","            look_at=\"/Root/Warehouse/SM_CardBoxA_3\",\n","        )\n","\n","# Create a render product with the configured camera\n","render_product = rep.create.render_product(camera, (512, 512))\n","\n","# Initialize the writer with specific parameters\n","writer = rep.WriterRegistry.get(\"KittiWriter\")\n","writer.initialize(\n","    output_dir=r\"C:\\\\Users\\\\mahmo\\\\work\",\n","    bbox_height_threshold=5,\n","    fully_visible_threshold=0.75,\n","    omit_semantic_type=True,\n","    colorize_instance_segmentation=True\n",")\n","\n","# Attach the render product to the writer\n","writer.attach([render_product])\n","\n","# Run the orchestrator to start the simulation\n","rep.orchestrator.run()"]},{"cell_type":"markdown","source":["The script above can be used to generate sssynthetic data once you load the correct scene from the guide provided"],"metadata":{"id":"V_H7wm2qRvjy"}},{"cell_type":"code","source":["labelformat convert `\n","    --task object-detection `\n","    --input-format kitti `\n","    --input-folder \"C:\\Users\\mahmo\\Simulated-world\\omniverse-ai\\Dataset\\object_detection\" `\n","    --category-names barel,bottle,box,bracket,bucket,cart,ceiling,cone,crate,emergency_board,fire_extinguisher,floor,floor_decal,lamp,pallet,paper_note,paper_shortcut,pillar,rack,sign,wall `\n","    --images-rel-path \"..\\rgb\" `\n","    --output-format coco `\n","    --output-file \"C:\\Users\\mahmo\\Simulated-world\\omniverse-ai\\COCO-labels\\output.json\"\n"],"metadata":{"id":"Vca5NeyCSEp4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["convert your kitti format labels to coco format"],"metadata":{"id":"LYkQfIqTSKkA"}},{"cell_type":"code","source":["# Import the necessary module for package installation\n","import omni.kit.pipapi\n","\n","# Install the transformers package\n","omni.kit.pipapi.install(package=\"transformers\")\n","\n","# Install the requests package\n","omni.kit.pipapi.install(package=\"requests\")\n","\n","print(\"Packages installed successfully.\")\n","#download packages in omniverse\n"],"metadata":{"id":"0ODuReRT2qIN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# omni.kit.pipapi extension is required\n","import omni.kit.pipapi\n","\n","# It wraps `pip install` calls and reroutes package installation into user specified environment folder.\n","# That folder is added to sys.path.\n","# Note: This call is blocking and slow. It is meant to be used for debugging, development. For final product packages\n","# should be installed at build-time and packaged inside extensions.\n","omni.kit.pipapi.install(\n","    package=\"pillow\",\n","    module=\"PIL\",  # the module name for import check\n","    ignore_import_check=False,\n","    ignore_cache=False,\n","    use_online_index=True,\n","    surpress_output=False,\n","    extra_args=[]\n",")\n","\n","# Verify installation\n","try:\n","    from PIL import Image\n","    print(\"Pillow installed successfully\")\n","except ImportError:\n","    print(\"Failed to install Pillow\")\n"],"metadata":{"id":"-mmLLcwP8GM9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["download necessary packges in omniverse env"],"metadata":{"id":"TyMcS_BL3XGn"}},{"cell_type":"code","source":["#Define the Hugging Face API endpoint and your model ID\n","APIURL = \"https://api-inference.huggingface.co/models/facebook/sam-vit-base\"\n","headers = {\"Authorization\": \"Bearer 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\"}\n","\n","#Function to segment an image\n","def segment_image(image_bytes, frame_number):\n","    response = requests.post(API_URL, headers=headers, files={\"file\": image_bytes})\n","    segmentation_result = response.json()\n","\n","    # Save or process the segmentation result\n","    with open(f\"segmentation_results/frame{frame_number}_segmentation.json\", \"w\") as result_file:\n","        json.dump(segmentation_result, result_file)\n","\n","    print(f\"Processed frame {frame_number}\")\n","\n","#Function to capture and process images\n","def capture_and_process_images(frame_number, image_array):\n","    image = Image.fromarray(image_array)\n","    image_bytes = io.BytesIO()\n","    image.save(image_bytes, format='PNG')\n","    image_bytes = image_bytes.getvalue()\n","\n","    # Create a new thread for each image segmentation\n","    threading.Thread(target=segment_image, args=(image_bytes, frame_number)).start()\n","#Create and configure the camera\n","camera = rep.create.camera()\n","with rep.trigger.on_frame(num_frames=100):  # Capture 100 frames\n","    with camera:\n","        rep.modify.pose(\n","            position=rep.distribution.uniform((-5, 5, 1), (-1, 15, 5)),\n","            look_at=\"/Root/Warehouse/SM_CardBoxA_3\",\n","        )\n","\n","#Create a render product with the configured camera\n","render_product = rep.create.render_product(camera, (512, 512))\n","\n","#Initialize the writer (not actually writing to disk, just capturing frames)\n","writer = rep.WriterRegistry.get(\"BasicWriter\")\n","writer.initialize(\n","    output_dir=\"/dev/null\",  # Using a dummy path as we're not actually saving images to disk\n",")\n","\n","#Attach the render product to the writer\n","writer.attach([render_product])\n","\n","#Function to capture frames and run inference\n","async def process_frames():\n","    frame_number = 0\n","    while frame_number < 100:  # Capture and process 100 frames\n","        frame = await rep.orchestrator.step_async()\n","        image_array = frame[\"images\"][render_product].numpy()\n","        capture_and_process_images(frame_number, image_array)\n","        frame_number += 1\n","\n","#Run the orchestrator to start the simulation and process frames\n","async def main():\n","    await rep.orchestrator.run_async()\n","    await process_frames()\n","    await rep.orchestrator.stop_async()\n","\n","#Execute the main function in the asyncio event loop\n","asyncio.run(main())\n","\n","print(\"Simulation and processing completed.\")"],"metadata":{"id":"z9esbbf3WY0P"},"execution_count":null,"outputs":[]}]}
